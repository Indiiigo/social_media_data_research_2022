{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTilER2yXIHK"
      },
      "source": [
        "# Collect Reddit Data using Pushshift.io\n",
        "\n",
        "adapted from: https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXnSIznfXIHO"
      },
      "source": [
        "Here we will demonstrate how to use pushift.io to collect Reddit data. We will first see what the website looks like and try to query it through the browser using an HTTP request. You can try copying and pasting \"https://api.pushshift.io/reddit/search/submission/\" into a browser and see what you get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDvlGCijXIHP"
      },
      "source": [
        "Now we will import some packages that will help us query pushshift programmatically. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yp6aeL_uXIHQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for data manipulation\n",
        "import requests # for executing the HTTP request\n",
        "import json # for data manipulation in JSON format\n",
        "import csv # for data manipulation in CSV format\n",
        "import time   # api's not designed for big data analysis so we need to regulate our requests\n",
        "import datetime # converting UNIX timestamps to human readable date formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7PlR48KXIHR"
      },
      "source": [
        "Next we will write a function (getPushSiftData) that builds and executes the HTTP request based on the parameters specified which in this case would be a keyword (query), a time stamp (after) after which we want data and the subreddit (sub)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hFDFqs-LXIHS"
      },
      "outputs": [],
      "source": [
        "def getPushshiftData(query, after, sub):\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&subreddit='+str(sub)\n",
        "    print(url)\n",
        "    r = requests.get(url)\n",
        "    data = json.loads(r.text)\n",
        "    return data['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwZSQgHVXIHT"
      },
      "source": [
        "The next function is a data helper which helps us organize and convert the JOSN data we get from pushshift into a dictionary so that we can work with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oqMDmkENXIHU"
      },
      "outputs": [],
      "source": [
        "def collectSubData(subm):\n",
        "    subData = list() #list to store data points\n",
        "    title = subm['title']\n",
        "    url = subm['url']\n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"    \n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
        "    numComms = subm['num_comments']\n",
        "    permalink = subm['permalink'] # remove later\n",
        "    \n",
        "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair))\n",
        "    subStats[sub_id] = subData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P49CJawhXIHV"
      },
      "source": [
        "Now we specify the two parameters, subreddit names and the query. We will keep the query blank for now since we want to collect all the posts in these subreddits. We are interested in collecting data from 3 different subreddits with different political ideologies so we use a list of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4H5tNZBVXIHV"
      },
      "outputs": [],
      "source": [
        "#Subreddit to query\n",
        "subs = ['politics', 'JoeBiden']\n",
        "query = \"\"\n",
        "#before and after dates https://www.unixtimestamp.com/index.php\n",
        "subStats = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWe8I_ppXIHW"
      },
      "source": [
        "Now, we finally come to the actual step where we call the different functions we had previously defined and call them to collect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "vCd66CNpXIHW",
        "outputId": "2eb82bc7-f29f-4b82-9f14-439b8985e7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=\t1667599156&subreddit=politics\n",
            "248\n",
            "2022-11-05 21:28:28\n",
            "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1667683708&subreddit=politics\n",
            "248\n",
            "2022-11-06 20:57:28\n",
            "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1667768248&subreddit=politics\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-19974e3ae651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPushshiftData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "all_data = []\n",
        "\n",
        "for sub in subs:\n",
        "    after = \"\t1667599156\"  \n",
        "    subCount = 0\n",
        "    data = getPushshiftData(query, after, sub)\n",
        "    # Will run until all posts have been gathered \n",
        "    # from the 'after' date up until before date\n",
        "    while len(data) > 0:\n",
        "        for submission in data:\n",
        "            collectSubData(submission)\n",
        "            subCount+=1\n",
        "            # Calls getPushshiftData() with the created date of the last submission\n",
        "        print(len(data))\n",
        "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "        after = data[-1]['created_utc']\n",
        "        data = getPushshiftData(query, after, sub)\n",
        "        time.sleep(10)\n",
        "    \n",
        "    print(len(data))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvikwF-9XIHX",
        "outputId": "ee1432bb-3760-4a63-ead3-71a86184511a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(subStats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFe7S0YzXIHY"
      },
      "source": [
        "Here are some statistics on the data that we collected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OW2d0tjXIHY",
        "outputId": "5c963ded-f534-4ebc-ceb9-ad43226743ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "496 submissions have added to list\n",
            "1st entry is:\n",
            "Faith on the ballot: White Christian nationalism vs. Black Christian tradition created: 2022-11-04 22:01:26\n",
            "Last entry is:\n",
            "Make America Truly Great...For the Very First Time created: 2022-11-06 20:57:28\n"
          ]
        }
      ],
      "source": [
        "print(str(len(subStats)) + \" submissions have added to list\")\n",
        "print(\"1st entry is:\")\n",
        "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
        "print(\"Last entry is:\")\n",
        "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMTrul9HXIHY",
        "outputId": "3fcdd6db-5692-4c6d-a5c8-ac8d07eea6a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('yo2imw',\n",
              " [('yo2imw',\n",
              "   'Make America Truly Great...For the Very First Time',\n",
              "   'https://www.commondreams.org/views/2022/11/06/make-america-truly-greatfor-very-first-time',\n",
              "   'Picture-unrelated',\n",
              "   1,\n",
              "   datetime.datetime(2022, 11, 6, 20, 57, 28),\n",
              "   1,\n",
              "   '/r/politics/comments/yo2imw/make_america_truly_greatfor_the_very_first_time/',\n",
              "   'NaN')])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "list(subStats.items())[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaMdOnfTXIHZ"
      },
      "source": [
        "Now we will save the data as a CSV file so that it is easy to manipulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js4HfcVtXIHZ",
        "outputId": "21b78bf4-0c42-42d2-a117-edb586032f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input filename of submission file, please add .csv\n",
            "elections.csv\n",
            "496 submissions have been uploaded\n"
          ]
        }
      ],
      "source": [
        "def updateSubs_file():\n",
        "    upload_count = 0\n",
        "    location = \"\"\n",
        "    print(\"input filename of submission file, please add .csv\")\n",
        "    filename = input()\n",
        "    file = location + filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\"]\n",
        "        a.writerow(headers)\n",
        "        for sub in subStats:\n",
        "            a.writerow(subStats[sub][0])\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" submissions have been uploaded\")\n",
        "\n",
        "updateSubs_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271ba9nTXIHZ"
      },
      "source": [
        "Other options: https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy5uMaC1XIHZ"
      },
      "source": [
        "Before we finish, let us upload the csv file with elections data that has already been collected for you and that we will analyze in the rest of the workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "0FcaBh8CXIHa",
        "outputId": "675f7b86-a982-4db8-b168-011c2bb11cf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Post ID                                              Title  \\\n",
              "0  ymbsdc  Faith on the ballot: White Christian nationali...   \n",
              "1  ymbsxo  Michigan governor's race: Whitmer leads Dixon ...   \n",
              "2  ymbt2l  The nightmarish Supreme Court case that could ...   \n",
              "3  ymbx4m  Gov. Newsom posthumously pardons abortion prov...   \n",
              "4  ymbxqt  ‘Dark money’ groups aligned with party leaders...   \n",
              "\n",
              "                                                 Url       Author  Score  \\\n",
              "0  https://www.bostonglobe.com/2022/11/04/opinion...   Adult-male      1   \n",
              "1  https://www.foxnews.com/politics/michigan-gove...   WarWolf343      1   \n",
              "2  https://www.vox.com/policy-and-politics/2022/1...      Streona      1   \n",
              "3  https://www.latimes.com/california/story/2022-...    misana123      1   \n",
              "4  https://www.opensecrets.org/news/2022/11/dark-...  artistpearl      1   \n",
              "\n",
              "          Publish Date  Total No. of Comments  \\\n",
              "0  2022-11-04 22:01:26                      1   \n",
              "1  2022-11-04 22:02:04                      1   \n",
              "2  2022-11-04 22:02:13                      1   \n",
              "3  2022-11-04 22:07:01                      1   \n",
              "4  2022-11-04 22:07:43                      1   \n",
              "\n",
              "                                           Permalink Flair  \n",
              "0  /r/politics/comments/ymbsdc/faith_on_the_ballo...   NaN  \n",
              "1  /r/politics/comments/ymbsxo/michigan_governors...   NaN  \n",
              "2  /r/politics/comments/ymbt2l/the_nightmarish_su...   NaN  \n",
              "3  /r/politics/comments/ymbx4m/gov_newsom_posthum...   NaN  \n",
              "4  /r/politics/comments/ymbxqt/dark_money_groups_...   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-658a8773-1329-45b4-9ea7-3ead9e2d2cce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Url</th>\n",
              "      <th>Author</th>\n",
              "      <th>Score</th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Total No. of Comments</th>\n",
              "      <th>Permalink</th>\n",
              "      <th>Flair</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ymbsdc</td>\n",
              "      <td>Faith on the ballot: White Christian nationali...</td>\n",
              "      <td>https://www.bostonglobe.com/2022/11/04/opinion...</td>\n",
              "      <td>Adult-male</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-11-04 22:01:26</td>\n",
              "      <td>1</td>\n",
              "      <td>/r/politics/comments/ymbsdc/faith_on_the_ballo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ymbsxo</td>\n",
              "      <td>Michigan governor's race: Whitmer leads Dixon ...</td>\n",
              "      <td>https://www.foxnews.com/politics/michigan-gove...</td>\n",
              "      <td>WarWolf343</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-11-04 22:02:04</td>\n",
              "      <td>1</td>\n",
              "      <td>/r/politics/comments/ymbsxo/michigan_governors...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ymbt2l</td>\n",
              "      <td>The nightmarish Supreme Court case that could ...</td>\n",
              "      <td>https://www.vox.com/policy-and-politics/2022/1...</td>\n",
              "      <td>Streona</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-11-04 22:02:13</td>\n",
              "      <td>1</td>\n",
              "      <td>/r/politics/comments/ymbt2l/the_nightmarish_su...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ymbx4m</td>\n",
              "      <td>Gov. Newsom posthumously pardons abortion prov...</td>\n",
              "      <td>https://www.latimes.com/california/story/2022-...</td>\n",
              "      <td>misana123</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-11-04 22:07:01</td>\n",
              "      <td>1</td>\n",
              "      <td>/r/politics/comments/ymbx4m/gov_newsom_posthum...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ymbxqt</td>\n",
              "      <td>‘Dark money’ groups aligned with party leaders...</td>\n",
              "      <td>https://www.opensecrets.org/news/2022/11/dark-...</td>\n",
              "      <td>artistpearl</td>\n",
              "      <td>1</td>\n",
              "      <td>2022-11-04 22:07:43</td>\n",
              "      <td>1</td>\n",
              "      <td>/r/politics/comments/ymbxqt/dark_money_groups_...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-658a8773-1329-45b4-9ea7-3ead9e2d2cce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-658a8773-1329-45b4-9ea7-3ead9e2d2cce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-658a8773-1329-45b4-9ea7-3ead9e2d2cce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "elections_df = pd.read_csv(\"elections.csv\")\n",
        "elections_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVQOwI0lXIHa",
        "outputId": "095a60ff-3b75-4a5a-fbac-02ca835b3435"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(elections_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gm9al4aXIHa"
      },
      "source": [
        "We would like to extract the subreddit name from the URL link using string manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GKDYDf2PXIHb"
      },
      "outputs": [],
      "source": [
        "elections_df['Subreddit'] = [i.split('/')[2] for i in elections_df['Permalink']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QQt7fbDEXIHb"
      },
      "outputs": [],
      "source": [
        "elections_df.to_csv(\"elections.csv\", encoding = 'utf-8', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DteUnxANXIHb"
      },
      "source": [
        "This notebook demonstrates how you can collect Reddit data in JSON format and save it as a CSV. Much of social media data like Twitter or news media data is avaliable as JSON\n",
        "Therefore, you can reuse what you learnt here to collect Twitter data. You would just have to use a different API but many of the same processes (rate limiting, data format) apply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyW-F72OXIHb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}